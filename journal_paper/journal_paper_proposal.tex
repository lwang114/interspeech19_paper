
%% bare_jrnl.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[journal]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
\usepackage{amsmath}
\usepackage{amsfonts}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}
\newcommand{\reals}{\mathbb R}
\newcommand{\bC}{\mathbf C}
\newcommand{\bI}{\mathbf I}
\newcommand{\ba}{\mathbf a}
\newcommand{\bd}{\mathbf d}
\newcommand{\bv}{\mathbf v}
\newcommand{\bw}{\mathbf w}
\newcommand{\bx}{\mathbf x}
\newcommand{\by}{\mathbf y}
\newcommand{\bz}{\mathbf z}

\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Multimodal Word Discovery and Retrieval with spoken descriptions and visual concepts}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

\author{Liming Wang,~\IEEEmembership{Member,~IEEE,}
        Mark Hasegawa-Johnson,~\IEEEmembership{Fellow,~IEEE,}
        %and~Jane~Doe,~\IEEEmembership{Life~Fellow,~IEEE}% <-this % stops a space
%\thanks{M. Shell was with the Department
%of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta,
%GA, 30332 USA e-mail: (see http://www.michaelshell.org/contact.html).}% <-this % stops a space
\thanks{J. Doe and J. Doe are with Anonymous University.}% <-this % stops a space
\thanks{Manuscript received April 19, 2005; revised August 26, 2015.}}

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.




% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
This paper demonstrates three systems capable of performing the multimodal word discovery tasks. A multimodal word discovery system accepts, as input, a database of spoken descriptions of images (or a set of corresponding phone transcriptions) and learns a lexicon which a mapping from a phone strings to their associated image concepts. Several systems are demostrated: two based on statistical machine translation (SMT), two based on neural machine translation (NMT). The systems are then compared with various existing unsupervised word discovery systems and speech-to-translation systems in the task of word discovery.  
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Unsupervised word discovery, machine translation, multimodal learning
\end{IEEEkeywords}






% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}
Unsupervised word discovery aims to segment and cluster spoken sentences or its corresponding phone transcriptions into word. It is useful for speech technology for unwritten language or language in which word segmentation and lexicon will be prohibitively expensive to acquire. Automatic word discovery system exists \cite{Bharadwaj2013, Rasanen2015, Kamper2017}, but the task has been shown to be quite challenging.     
% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps.
% 
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
% 
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by the IEEE):
% \IEEEPARstart{A}{}demo file is ....
% 
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
% 
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.


%\IEEEPARstart{T}{his} demo file is intended to serve as a ``starter file''
%for IEEE journal papers produced under \LaTeX\ using
%IEEEtran.cls version 1.8b and later.
% You must have at least 2 lines in the paragraph with the drop letter
% (should never be an issue)
%I wish you the best of success.

%\hfill mds
 
%\hfill August 26, 2015
\section{Related works}
\section{Problem formulation}
Suppose we have a sequence of audio feature frames $x_1, \ldots, x_{T_x}, x \in \mathcal X$ and a set of image concepts $y_1, \ldots, y_{T_y}, y \in \mathcal Y \cup \{NULL\}$. The goal of our algorithm is a sequence learning problem to align every audio feature frames $\mathbf x = [x_1, \ldots, x_{T_x}]$ to an image concept $\mathbf y = [y_1, \ldots, y_{T_y}]$. We assume that $T_x > T_y$. In other words, we seek an alignment matrix $A \in [0, 1]^{T_x \times T_y}=[\mathbf a_1^\top, \ldots, \mathbf a_{T_x}^{\top}]^{\top} = [\tilde{\mathbf a}_1 \ldots \tilde{\mathbf a}_{T_y}]$. A key assumption of our model is the constraint:
\begin{align}\label{eq:one-concept-assumption}
    \sum_{j=1}^{T_y} a_{ij} = 1, i = 1, \ldots, T_x,
\end{align}
which ensures that one feature frame is aligned to only one concept. Therefore our model tries to maximize:
\begin{align}\label{eq:trans_prob}
    p(\mathbf{x}|\mathbf{y}) = \sum_{\mathbf{A}\in \{0, 1\}^{T_y  \times T_x}} p(\mathbf{A}|\mathbf{y}) p(\mathbf{x}|\mathbf{y}, \mathbf{A}),
\end{align}
for each input-concept sequence pair. When $\mathcal X$ is a finite set of phonetic symbols for the spoken language, the problem is refered later as \textit{phone-level} word discovery; when $\mathcal X = \reals^{D}$, where $D$ is the dimension of the acoustic feature such as ceptral features, bottleneck features or other acoustic embeddings, the problem is refered as the \textit{audio-level} word discovery.

\section{Models for phone-level multimodal word discovery}
\subsection{Statistical machine translation models}
\subsubsection{Mixture model}
Statistical machine translation optimizes \ref{eq:trans_prob} by learning the probabilities $p(\mathbf{y}|\mathbf{x}, \mathbf{A})$ and $p(\mathbf{A}|\mathbf{x})$. For the phone-level word discovery, following \cite{Brown92}, we can make use of the following assumptions:
\begin{enumerate}
    \item $\mathbf{A}$ is integer-valued, specifically $A_{it}=1$ for $i=i(t)$, else $a_{it}=0$. For notational convenience, let $a_t := A_{i(t) t}$; 
    \item all alignments are equally likely given only $\mathbf{y}$: $p(\mathbf A|\mathbf y) = \frac{\epsilon}{(T_y+1)^{T_x}}$, where $\epsilon$ is some normalization constant;
    \item given the alignment,
%$\forall y_j, j\neq a_i$ is conditionally independent of $x_i$ and $x_i$'s are independent of $x_j, j\neq i$ given $y_{a_i}$:$p( \mathbf{x}|\mathbf{y}, \mathbf{a}) = \prod_{i=1}^{T_x} p(x_i|x_{1:i-1}, y_{a_i}) = p(x_i|y_{a_i})$.
each phone depends only on its aligned image concept, thus
$p(x_t|x_{1:(t-1)},\mathbf{A},\mathbf{y})=p(x_t|y_{i(t)})$,
\end{enumerate}
Eq. (\ref{eq:smt_trans_prob}) is then simplified to:
\begin{align}
%    &\frac{\epsilon}{(T_y+1)^{T_x}}\sum_{a_1 = 0}^{T_y} \sum_{a_2 = 0}^{T_y}\ldots \sum_{a_{T_x} = 0}^{T_y} 
%    \prod_{i=n}^{T_x}p(x_i|y_{a_i}).
    &\frac{\epsilon}{(T_y+1)^{T_x}}\prod_{t=1}^{T_x}\sum_{i(t)=1}^{T_y} p(x_t|y_{i(t)}).
\end{align}
Model under such assumptions are called \cite{mixture model} mainly due to the independence between alignments. Optimization with EM results in 
%which can be optimized with EM algorithm and result in
an iterative formula
in terms of the expected counts of a given phone-concept pattern.

The optimal alignment between the phones and image concepts can be then obtained by finding the highest-scored translation pair of a given sentence:
\begin{align}\label{eq:smt_alignment}
    i^*(t) = \arg\max_i p(x_t|y_i).
\end{align}

% needed in second column of first page if using \IEEEpubid
%\IEEEpubidadjcol

\subsubsection{Extensions of mixture model}
% IBM 2
\cite{Brown92} has proposed several extensions to the mixture model. First, they relaxes the second assumption by making the alignment probability time-dependent. Specifically they introduce a set of parameters $a(i|t, T_x, T_y) \approx p(a_{ti} = 1|\mathbf x_{1:t}, \mathbf y_{1:i}, T_x, T_y), i\in \{1,\ldots,T_y\}, t\in \{1,\ldots,T_x\}$ with the constraint that $\sum_{i=1}^{T_y} a(i|t, T_x, T_y) = 1$. As a result:
\begin{align}\label{eq:align_prob_ibm2}
    p(A|\mathbf x, \mathbf y)& = \prod_{t=1}^{T_x} a(i(t)|t, T_x, T_y)
\end{align}
, where $i(t)$ is the index for the aligned concept for feature frame $x_t$. A formula similar to the mixture model can be used to update the parameters. 

% discuss more about IBM 3, 4, 5 

% HMM model
\subsubsection{Hidden Markov model}
An alternative way to relax the second assumption is to add conditional dependence between alignments at each time. This is motivated by several issues with the mixture model. First, when the second assumption holds, the alignments at each time are independent, identically distributed (i.i.d) but this will make the phone sequence of any arbitrary order to have the same probability. For example, the word ``god'' and ``dog'' will have the same probability given an image concept. The position-based alignment probabilities break this limitation but fails to consider that in word discovery, the alignment of ``cat'' stays the same regardless of its position. Another issue is that the independence assumption 3) tends to make the alignment ``fragmental'' since it fails to account for the fact that sequences nearby are more likely to align to the same object. In other words, a better model should take into account the conditional dependence between alignments. For example, we may make the Markov assumption that $p(a_t|\mathbf a_{1:t-1}) = p(a_t|a_{t-1})$ and 
%Therefore, following \cite{Vogel1996}, we introduce the parameters $\pi(i|T_y) \approx p(\mathbf a_{1, i}=1|T_y)$ and $r(|j-i||T_y) \approx p(\mathbf a_{t,j}=1|\mathbf a_{t-1, i}=1, T_y)$, subject to the constraints that $\sum_{k=1}^{T_y}\pi(k) = 1, \sum_{k=0}^{T_y-1} r(k|T_x, T_y) = 1$, which can be treated as the initial and transition probabilities of a Markov chain. Eq. The model then again tries to maximize (\ref{eq:trans_prob}), which in this case simplifies to:
\begin{align}\label{eq:align_prob_hmm}
    p(\mathbf x|\mathbf y, \mathbf A) &= \frac{\epsilon}{(T_y+1)^{T_x}}\prod_{t=0}^{T_x} p(a_{t+1}|a_t) p(x_{t+1}|y_{a_{t+1}}),
\end{align}
where $p(a_1|a_0) := p(a_1)$. This model, as first shown by \cite{Vogel1996}, is a hidden Markov model with alignment vectors as the states. 

Plug Eq. (\ref{eq:align_prob_hmm}) into Eq. (\ref{eq:trans_prob}) and apply standard forward-backward algorithm for HMM \cite{Rabiner89-ATO}, it can be shown that:
\begin{align}
\label{eq:dhmm_em_update}
    \hat p(i) &= \frac{\sum_{t=1}^{T_x}\alpha_t(i)\beta_t(i)}{\sum_{t=1}^{T_x}\sum_{i=1}^{T_y}\alpha_t(i)\beta_t(i)}\\
    \hat p(j|i) &=  \frac{\sum_{t=1}^{T_x}\alpha_t(i)p(j|i)p(x_{t+1}|y_{a_{t+1}})\beta_t(i)}{\sum_{t=1}^{T_x}\sum_{i=1}^{T_y}\alpha_t(i)p(j|i)p(x_{t+1}|y_{a_{t+1}})\beta_t(j)}\\
    \hat p(y_i| x_t) &= \frac{\sum_{t=1}^{T_x}\gamma_t(i)}{\sum_{t=1}^{T_x}\sum_{i'=1}^{T_y}\gamma_t(i')},
\end{align}
for all $t \in \{1, \cdots, T_x\}, i, j \in \{1, \cdots, T_y\}$ and all pairs of phonemes and concepts, where $\alpha_t(i) := p(\mathbf x_{1:t}, a_t=i|\mathbf y)$, $\beta_t(i) := p(\mathbf x_{t+1:T_x}| a_t=i)$. $\gamma_t(i) := p(\mathbf x, a_t=i|\mathbf y) = \alpha_t(i) \beta_t(i)$ is the expected count for the alignment at time $t$ to concept $i$. One simplification of the model mentioned in \cite{Vogel1996} is to add the constraint that $p(j|i) = q(j-i)$ since the alignment of the current state depends only on the relative change of aligned position with respect to the previous alignment. For multimodal word discovery, since the order of the concept does not matter, the number of parameters for the transition probability can be further reduced by $p(j|i) = q \delta_{ij} + (1-q)(1-\delta_{ij})$, where $\delta_{ij} = 1$ if $i=j$ and 0 otherwise. In other words, the alignment of the current phone depends only on whether it aligns to the same concept as the previous phone or not. 

standard Viterbi decoding can be applied to find the optimal alignment between concept and phone: 
\begin{align*}
    a_t &= \max_{1 \leq i \leq T_y}\{p(\mathbf x_{1:t-1}, \mathbf a_{1:t-1}|\mathbf y) p(i|a_{t-1})p(x_t|y_i)\\
\end{align*}

\subsection{Neural machine translation models}
Instead of learning $p(\mathbf x|\mathbf y)$, the NMT model tries to learn the posterior probability:
\begin{align}\label{eq:nmt_trans_prob}
    p(\mathbf y|\mathbf x) &= \sum_{\mathbf A \in \mathcal A} p(A|\mathbf x) p(\mathbf y|\mathbf x, \mathbf A),
\end{align}
for each input phone-concept pair. 

Suppose the several assumptions are satisfied:
\begin{enumerate}
    \item There is a ``dominant'' alignment $\mathbf A^*$ such that $p(\mathbf A^*|\mathbf x) \approx 1$;
    \item The input representation of the phone $x_t$ can be compressed into a lower-dimensional embedding $h_t(x_t) =: \mathbf h_t$, $\forall t \in \{1, \cdots, T_x\}$. Let $\mathbf h = [\mathbf h_1, \cdots, \mathbf h_{T_x}]$;
    \item There exists some ``context'' vector $\mathbf c_i(\mathbf h, A^*), i=\{1, \cdots, T_y\}$ such that $y_i$ is conditionally independent of $\mathbf h$ given $\mathbf c_i$.
    \item There exists some ``state'' vector $\mathbf s_i(\mathbf y_{1:i-1})$ such that $y_i$ is independent of $\mathbf y_{1:i-2}$ given $\mathbf s_i$ and $y_{i-1}$.
\end{enumerate}

By sequentially applying the assumptions above,Eq. (\ref{eq:nmt_trans_prob}) is then simplified to one term:
\begin{align}\label{eq:nmt_trans_prob_simplify}
    p(\mathbf y|\mathbf x, \mathbf A)&= \prod_{i=1}^{T_y} p(y_i|y_{1:i-1}, \mathbf{x}, \mathbf{A}^*) = \prod_{i=1}^{T_y} p(y_i|\mathbf y_{1:i-1}, \mathbf h, \mathbf{A}^*)\\
    &= \prod_{i=1}^{T_y} p(y_i|\mathbf y_{1:i-1}, \mathbf c_i)\\
    &= \prod_{i=1}^{T_y} p(y_i|y_{i-1}, \mathbf s_i, \mathbf c_i).
\end{align}
In the standard attention \cite{Bahdanau14}, the dominant alignment is learned via a soft alignment:
\begin{align}\label{eq:def_soft_align}
    A_{it}:= \alpha_{it} = \frac{\exp(e_i(\mathbf h(\mathbf x_t), \mathbf s_{i-1})/T)}{\sum_{j=1}^{T_y} \exp(e_j(\mathbf h(\mathbf{x}_t), \mathbf s_{j-1})/T)}
    \mathbf{c}_i &= \sum_{t=1}^{T_x} a^*_{it} \mathbf{h}_{t},
\end{align}
where $\mathbf e(\cdot)$ can be learned by a feedforward neural network and $\mathbf s_i$ can be treated as the decoder state in a typical encoder-decoder architecture. The softmax ensures Eq. (\ref{eq:one-concept-assumption}) is satisfied and $T$ is a temperature term to smooth the softmax. The concept vectors are learned by:
\begin{align}\label{eq:def_context_vec}
    \mathbf c_i = \sum_{t=1}^{T_x} \alpha_{it}^* \mathbf h_t.
\end{align}
The assumption (3) of this section can be viewed as a soft version of the SMT assumption (3) in the previous section:  in the SMT model, concept $y_i$ depends only on the phones that align to it, which is equivalent to the special case when $\alpha_{it}^*$ in Eq.\label{eq:def_context_vec} is either 0 or 1. Therefore, the set of probabilities $\{p(y_i|y_{i-1}, \mathbf s_{i-1}, \mathbf c_i)\}_{i=1}^{T_y}$ can be learned using a recurrent neural net $f$ with state vector $s_i$. The problem then boils down to learning the functions $h, e, f$ such that the log-likelihood of the concepts given the phone sequence is maximized.

For the multimodal word discovery model, special modification can be made on the attention mechanism. One way is to notice that since the order of the image concepts do not matter, the recurrent connection can simply be removed and the 
% Talk about the modification on the standard encoder-decoder framework

\section{Audio-level multimodal word discovery}
In the audio level, instead of phone sequence, we are given a sequence of acoustic feature vectors $\{\mathbf x_t\}_{t=1}^{T_x}$.  
% Linear Gaussian model for bottleneck features
Since features such as spectrogram or cochleagram tend to be high-dimensional, one essential step is to compress the audio features into lower dimensional space. Two main alternatives exist: one way is to employ knowledge in speech science to find a data-independent subspace, such as the mel-frequency subspace used in the MFCC (cite needed) and its variants; another way is to find a data-dependent subspace using statistical models such as linear Gaussian models (cite needed) or neural networks (cite needed). The second approach is generally known as the \textit{bottleneck features}. The two approach can often be combined by, for example, extracting bottleneck features from MFCC.

A next essential step is to model the probability distribution of the acoustic vectors. There are again two alternatives: One is to approximate the density function by a mass function by clustering the acoustic features into discrete units; the other is to learn the continuous distribution directly. The second approach is more theoretically sound and thus will be the focus of this paper. It turns out that the second approach allows a natural extension of both the statistical models \cite{Brown92} and neural models \cite{Bahdanau14} to model problems where the source language is continuous, as in the case of multimodal acoustic word-unit discovery and speech-to-translation.

\subsection{Acoustic feature extraction}
the continuous features into phone-like symbols and apply the discrete models to the discretized sequence. To account for richer context, bottleneck features extracted from existing phone classification system or multilingual ASR system can be used (cite needed). 

The classical bottleneck feature can be viewed as the following Gaussian process:
\begin{align}\label{eq:bnf_gp}
    \mathbf z &\sim \mathcal N(0, \mathbf I)\\
    \mathbf w &\sim \mathcal N(0, \mathbf \Sigma)\\
    \mathbf x &= \mathbf C \mathbf z + \mathbf w,
\end{align}
where $\mathbf C \in \reals^{D \times B}$, and $B < D$ is the dimension of the bottleneck feature. The objective is to find $(\mathbf C, \mathbf\epsilon)$ such that $p(\mathbf x|\mathbf C, \epsilon)$ is optimized. Use the Baum-Welch auxiliary function:
\begin{align}\label{eq:lg_bnf_objective}
    &\max_{\bC, \epsilon} \mathbb E[\log p(\bx|\bz, \bC, \epsilon)|\bx, \bar\bC, \bar\epsilon]\\
    =&\max_{\bC, \Sigma}\mathbb E[-\frac{1}{2}\log (2\pi)^{B}|\Sigma| - \frac{1}{2}(\bx - \bC\bz) \Sigma^{-1}(\bx - \bC\bz)|\mathbf x, \bar\Theta].
\end{align}
As a result, the parameters of following iterative update:
\begin{align}
    \hat{\bC} &= x \mu_{\bz|\bx}^\top(\Sigma_{\bz|\bx} + \mu_{\bz|\bx}\mu_{\bz|\bx})^{-1}\\
    \hat{\Sigma} &= \mathbf E[(\bx - \bC \bz)(\bx - \bC \bz)^\top|\bx, \bar\Theta]\\
    &= \bx \bx^\top - \bC \mu_{z|x} \bx^\top,
\end{align}
where $\mu_{z|x} := \bC^\top(\bC^\bC^\top + \Sigma)^{-1} \bx, \Sigma_{\bz|\bx} = \bI - \bC^\top(\bC^\bC^\top + \Sigma)^{-1}\bC$. The bottleneck feature is then:
\begin{align}\label{eq:lg_bnf}
    f_{bnf}(x) = \mu_{z|x} = \bC^\top(\bC^\bC^\top + \Sigma)^{-1} x.
\end{align}

\subsection{SMT models}
 % Continuous IBM model
\subsubsection{Framewise Approach}
The audio-level SMT models replace the discrete translation probabilities $p(x_t|y_j)$ with probability density functions (PDF). Intuitively, it is clear that a desirable PDF should contain more than one ``peak'' density value since most words contain more than one phone or syllable; even the acoustic feature for word with a single syllable may contain more than one peak due to the variation of the speech production process. Notice that we largely ignored the speaker variation in the subsequent discussion and assumed proper speaker adaptation has been applied before the processing of our models.

Therefore, one desirable choice is to use a Gaussian mixture distribution for each image concept: 
\begin{align}\label{eq:gmm_prob}
    f(x_t|y_j) = \sum_{i=1}^{M} c_i(y_j) N(x_t|\mu_{y_j}, \Sigma_{y_j}),
\end{align}
where $\{c_i(j)\}_{i=1}^M$ is a prior probability distribution for each clusters associated with concept $y_j$ with $\sum_{i=1}^M c_i(j) = 1, \forall j \in \{1, \cdots, T_y\}$ and $N(x_t|\mu_{y_j}, \Sigma_{y_j}) := \frac{1}{((2\pi)^d |\Sigma_{y_j}|)^{1/2}}\exp\left(-\frac{1}{2}(\mathbf x_t - \mathbf \mu_{y_j})^\top\Sigma_{y_j}^{-1}(\mathbf x_t - \mathbf \mu_{y_j})\right)$. Under the same assumption (1)-(3) as the discrete mixture model, the continuous model seeks to maximize the \textit{Baum-Welch auxiliary function}:
\begin{align}\label{multimodal_gmm_objective}
&L(\bar{\Theta}, \Theta) = \sum_{t=1}^{T_x}\sum_{a_t=1}^{T_y}\sum_{z_t=1}^M p(a_t,z_t|\mathbf x_t, \mathbf y, \bar {\Theta}) \log f(\mathbf x_t, a_t | \mathbf y, \Theta)\\
&= \sum_{t=1}^{T_x}\sum_{a_t=1}^{T_y}\sum_{z_t=1}^M p(a_t, z_t|\mathbf x_t, \mathbf y, \bar{\Theta})(-\frac{1}{2}\log (2\pi)^d|\Sigma_{y_i}(m)| + \nonumber\\
&\log c_{z_t}(y_i) - \frac{1}{2} (\mathbf{x}_t - \mathbf \mu_{y_j}(m))^{\top}\Sigma_{y_j}(m)^{-1}(\mathbf{x}_t - \mathbf \mu_{y_j}(m))).
\end{align}
Although not the logarithm of the translation probability as in the discrete case, this auxiliary function is know to provide a lower bound for the log-translation probability and guarantees to converge to a local optimum. Further, similar to the discrete case, the model keeps track of the expected counts $\gamma_t(i, m) := p(a_t=i, z_t=m|\mathbf x, \mathbf y, \bar \Theta)$ and the mixture priors, means and variance of the GMM are updated as follows: 
% Continuous HMM model
\begin{align}\label{eq:cont_gmm_em_update}
    \hat c_m(i) &= \frac{\sum_{t=1}^{T_x}\gamma_t(i, m)}{\sum_{t=1}^{T_x}\sum_{m=1}^{M}\gamma_t(i, m)}\\
    \hat \mu_m(i) &= \frac{\sum_{t=1}^{T_x}\gamma_t(i, m)x_t}{\sum_{t=1}^{T_x}\sum_{m=1}^{M}\gamma_t(i, m)}\\
    \hat \Sigma_m(i) &= 
    \frac{\sum_{t=1}^{T_x}\sum_{m=1}^{M}\gamma_t(i, m)(\mathbf x_t - \mathbf \mu_t)(\mathbf x_t - \mathbf \mu_t)^\top}{\sum_{t=1}^{T_x}\sum_{m=1}^{M}\gamma_t(i, m)}.
\end{align}
The continuous HMM model can be similarly derived from the discrete one with an GMM as the emission probability for each image concept. 

The GMM model can be simplified into a KMeans-based clustering model by removing the covariance parameters and modeling each image concept $y_i$ as a set of clusters with centroid $\{\mu_{y_i}^m\}_{i=1, m=1}^{T_y, M}$. Instead of maximizing the likelihood function, the model tries to minimize:
\begin{align}\label{eq:multimodal_kmeans_obj}
    \min_{\mathbf a \in \mathcal A, \mathbf m \in [M]^{T_x}} \sum_{t=1}^{T_x} \|\mathbf x_t - \mu_{y_{a_t}}^{m_t}\|^2,
\end{align}
which can be seen as an extreme-version of Eq. (\ref{eq:multimodal_gmm_objective}) where the variances of the data for a given cluster goes to 0. The update is similar to the standard KMeans algorithm but guided by the image concepts corresponding to each utterances:
\begin{align}\label{eq:multimodal_kmeans_update}
    a_t^*, m_t^* &= \arg\min_{i, m}\|\mathbf x_t - \mu_{y_{i}}^{m}\|^2\\
    \mu_{y_i}^m &= \frac{1}{|\{t: a_t=i, m_t=m\}|}\sum_{t: a_t=i, m_t=m} \mathbf x_t.
\end{align}

Instead of modeling the individual feature frames as cluster, which may overlap across different word unit and leads to over-segmentation, it is wise to consider modeling a fixed context window around each frame or modeling the correlation across multiple frames using HMM-based model.

\subsubsection{Segmental Approach}
While most clustering techniques require data to be fixed-length, the word discovery inherently requires modeling variable-length context for each word. Existing works on acoustic word discovery has modeled variable-length context by explicitly modeling the segmentation in the generative process. However, since the word segmentation is generally assumed to be not directly observable in the acoustic feature, certain prior assumption about the word length distribution is often used. \cite{Lee2012} models the length probability as:
\begin{align}\label{eq:len_prob_seg_hmm}
    p(l_i=l|\mathbf z_i, l_1^{i-1}) = &\frac{(1 - \alpha_{z_i})^l \alpha_i}{\sum_{l'=1}^{T_x - \sum_{i'=1}^{i-1}l_{i'}}(1 - \alpha_i)^l \alpha_i},\\
    1\leq l \leq T_x - \sum_{i'=1}^{i-1}l_{i'},
\end{align}
which is simply the length distribution of $i-th$ segment starting with $1$ and continuing with $0$ of a binary string generated by $i.i.d$ Bernoulli random variables with parameter $\alpha_{z_i}$. 

The frame-level mixture models can also be viewed as a segmental approach where the length probability of the segments are generated by a fixed distribution that has the same form of Eq. (\ref{eq:len_prob_seg_hmm}):
\begin{align}\label{eq:len_prob_seg_mixture_smt}
    p(l_i=l|\mathbf a_i, l_1^{i-1}) = &\frac{\frac{1}{T_y+1}^l (1-\frac{1}{T_y+1})}{\sum_{l'=1}^{T_x - \sum_{i'=1}^{i-1}l_{i'}}(\frac{1}{ T_y+1})^l(1-\frac{1}{T_y+1})}, \\
    1\leq l \leq T_x - \sum_{i'=1}^{i-1}l_{i'},
\end{align}
which especially when $T_y$ is large, tends to generate shorter segments.

To make less assumption about the word length probability distribution, one may be inspired by the fertility-based approach used in \cite{Brown92} and define fertility probabilities $p(l|z)$ for each word cluster. The posterior probability for the word cluster $z_i$ is then:
\begin{align}\label{eq:post_prob_fertility_smt}
    p(z_i=j|\mathbf x) &= \sum_{n=1}^{T_x}\sum_{l_1^n} p(z_i=j, l_1^{n}|\mathbf x)\\
    &= \frac{\sum_{n=i}^{T_x}\sum_{t_i^1, l_i} p(z_i=j) p(\mathbf x_{t_i^1:t_i^1+l_i}|z_i=j)}{\sum_{n'=1}^{T_x}\sum_{l_1^{n'}'}\sum_{j'=1}^K p(z_i=j') p(\mathbf x_{t_i^1':t_i^1'+l_i'}|z_i=j')}. 
\end{align}

However, exact computation of of Eq. (\ref{eq:post_prob_fertility_snt}) turns out to be too expensive to compute. Approximate computation of the probability is, however, possible by making use again of the ``dominant path'' approximation. suppose there is an dominant segmentation $l_1^n$ with $p(l_1^n|\mathbf x)\approx 1$. Eq. (\ref{eq:post_prob_fertility_smt}) can be then approximated as:
\begin{align}\label{eq:post_prob_seg_smt}
   &\frac{\sum_{n=1}^{T_x}\sum_{l_1^n} p(z_i=j) p(\mathbf x_{t_i^1:t_i^1+l_i}|z_i=j)}{\sum_{n'=1}^{T_x}\sum_{l_1^{n'}'}\sum_{j'=1}^K p(z_i=j') p(\mathbf x_{t_i^1':t_i^1'+l_i'}|z_i=j')} \\
   \approx & \frac{p(z_i=j)p(\mathbf x_{t_i^{1*}:t_i^{1*}+l_i^*}|z_i=j)}{\sum_{j'=1}^K p(z_i=j) p(\mathbf x_{t_i^{1*}:t_i^{1*}+l_i^*}|z_i=j')}.
\end{align}
In fact, the segmental-embedded GMM in \cite{Kamper17} can be interpreted as using such an approximation with $p(\mathbf x_{t_i^1:t_i^1+l_i}|z_i=j)\propto \mathcal N(f(\mathbf x_{t_i^1:t_i^1+l_i});\mu_j, \sigma^2\mathbf I)^{l_i}$ and $l_1^{n*}$ is the Viterbi segmentation obtained by solving a dynamic program. 

\subsection{NMT models}

\section{Multimodal word discovery with audio and image regions}
Theoretically, the multimodal framework can also be extended to deal directly with audio $x_1, \cdots, x_{T_x}$ and image regions containing single objects $y_1, \cdots, y_{T_y}$. In this case, again we assume there is one object for each image concept in each example but no longer know if two image regions from two examples correspond to the same object. There is also no knowledge about the total number of objects. The SMT model in this case combines the formalism of SMT Canonical Component Analysis (CCA) models.
\subsection{SMT model}
Analogous to the generative process for the discrete mixture model case, the audio-regional mixture model generates the image regions and audio as follows:
\begin{align}\label{eq:audio_region_gp}
    &a_t \sim \mathbf{Categorical}(\frac{1}{(1+T_y)}, \cdots, \frac{1}{(1+T_y)})\\
    &\pi \sim \mathbf{Dir}(\alpha)\\
    &c_i \sim \pi\\
    &\by_i \sim \mathcal N(\mu_{c_i}, \Sigma_{c_i}^\top), i=1,\cdots,T_y\\
    &n_i \sim \mathcal N(0, \Sigma_{n, c_i})\\
    &\bx_t = A \by_{a_t} + \bn, t=1,\cdots,T_x.
\end{align}
However, the EM-algorithm for this generative process will involve nonlinear coupling of the parameters for the audio and image regions. Instead, we employ an \textit{reparameterization} by introducing $\bz \sim \mathcal N(0, \bI)$ and $(C_x^i, C_y^i), i=1,\cdots,K$ such that $\bx_t := C_x^{c_{a_t}} \bz + \bw_i$ and $\by_i := C_y^{c_i}\bz + \bv_i$ for some normal noise variables with zero-mean and covariance $\Sigma_w, \Sigma_v$ respectively. This is equivalent to the original process by letting $A = C_x^{(c_{a_t})}C_y^{(c_{a_t})\top} (C_y^{c_{a_t}}C_y^{c_{a_t}}^\top+\Sigma_w)^{-1}, \Sigma_{n, c_i} = \bI - C_y^{(c_{a_t})\top} (C_y^{c_{a_t}}C_y^{c_{a_t}}^\top+\Sigma_w)^{-1}C_y + \Sigma_v$. 

% More about the equivalence
this serves to decouple the expression inside the expectation. The generative process now becomes:
\begin{align}\label{eq:audio_region_repara_gp}
    &a_t \sim \mathbf{Categorical}(\frac{1}{(1+T_y)}, \cdots, \frac{1}{(1+T_y)})\\
    &\pi \sim \mathbf{Dir}(\alpha)\\
    &c_i \sim \pi\\
    &\bz_i \sim \mathcal N(0, \bI)\\ 
    &\bw_i \sim \mathcal N(0, \Sigma_{w, c_i}), \bv_i \sim \mathcal N(0, \Sigma_{v, c_i})\\
    &\by_i \sim C_y^{c_i}\bz_i + \bw_i, i=1,\cdots,T_y\\
    &\bx_t = C_x^{c_i} \by_{a_t} + \bv_i, t=1,\cdots,T_x.
\end{align}

The posterior probability:
\begin{align*}
    p(\mathbf z|\mathbf x, \mathbf y) = \mathcal N(\mathbf z; \mu_{\mathbf z|\mathbf x, \mathbf y}, \Sigma_{\mathbf z|\mathbf x, \mathbf y}),
\end{align*}
where $\mu_{\mathbf z|\mathbf x, \mathbf y} := C_x^\top \Sigma_x^{-1}\mathbf x + C_y^\top \Sigma_y^{-1}(\mathbf y - C_y C_x^\top \Sigma_x^{-1}x), \Sigma_{\mathbf z|\mathbf x, \mathbf y} := I - C_x\top \Sigma_x^{-1} C_x $(continue).

\section{Prior works: unsupervised word discovery and speech-to-text tranlation}

\subsection{Unsupervised word discovery}
% Discussion of Lee & Glass's model
The first unsupervised word/subword discovery system was proposed by \cite{Lee2012}. Their model used the following generative process:
% TODO: two equations per line
\begin{align}\label{eq:generate_subword_hmm}
    \mathbf \pi &\sim \mathbf{Dir}(\gamma)\\
    \mathbf \theta_c &\sim \mathbf{Dir}(\theta_0), c \in \{1, \cdots, K\}\\
    \mathbf b_t &\sim \mathbf{Bernoulli}(\alpha_b), t \in \{1, \cdots, T\}\\
    d_i &= \left(\min_{t>t_{i-1}:b_t=1} t\right) - t_{i-1}\\
    c_i &\sim \mathbf \pi, i \in \{1, \cdots, L\}\\
    s_t &\sim \mathbf a^{c_i, s^{t-1}}\\
    \mathbf m_t &\sim \mathbf w^{c_i, s_t}\\
    \mathbf x_t &\sim \mathcal N(\mathbf \mu^{c_i, s_t}_{m_t}, (\mathbf \lambda^{c_i, s_t}_{m_t})^{-1}\mathbf I), \\
    t &\in \{t_{i-1}+1, \cdots, t_{i-1}+d_i\},
\end{align}
where $\mathbf \theta_c = [\mathbf a^{c_i}, \mathbf w^c, \mathbf \mu_t^{c}, \mathbf \lambda_i^c]$ and $t_{-1} = 0$, $\mathbf{Dir}(\theta)$ is the Dirichlet prior distribution with concentration parameter $\theta$ and $\mathbf{Bernoulli}(\alpha_b)$ is the Bernoulli distribution with probability $\alpha_b$ to be 1. Intuitively, the model first generates $K$ HMMs to model each subword cluster, and then for each utterance $\mathbf x$ it first generates a segmentation as specified by the duration vector $\mathbf d$ and model each segment of length $d_i$ with an HMM for cluster $c_i$. The model is trained using EM algorithm with Gibbs sampling. To infer the cluster id of each frame of the utterance, the model needs to find $\mathbf d, \mathbf c$ such that $p(\mathbf x, \mathbf d, \mathbf c)=p(\mathbf x, \mathbf d)p(\mathbf c|\mathbf x, \mathbf d)$  is maximized, which amounts to evaluate the posterior probability $p(\mathbf d_i, \mathbf x)$ and $p(\mathbf c_i|\mathbf x, \mathbf d)$. Due to prohibitive computational complexity to evaluate the expressions exactly, \cite{Lee2012} proposed to approximate the posterior with Gibbs sampling. The inference can be then essentially broken into two problems: first, finding an optimal segmentation of the utterance; second, finding the optimal cluster assignment given the segments of the utterance. The second problem is straightforward and amounts to finding the maximal posterior probabilities of the clusters; the first problem can be solved using a dynamic program with $\delta(t) := p(\mathbf y_{1:t}, z_{1:k(t)})$, where $k(t)$ is the cluster assigned to frame at time $t$:
\begin{align}\label{eq:subword_hmm_dp}
    \delta(t) &= \max_{\tau}\{\delta(t-\tau) (\sum_{c=1}^K\pi_c \sum_{i=1}^{N}\alpha^c_{\tau}(i))\} \\
    \phi(t) &= t - \arg \max_{\tau}\{\delta(t-\tau)\sum_{c=1}^K\pi_c \sum_{i=1}^{N}\alpha^c_{\tau}(i)\} \\
\end{align}
where $N$ is the number of state of the HMMs and $\alpha^c_(t)$ is the forward probability of HMM for cluster $c$. The end boundaries for the optimal segmentation is then $[1, \cdots, \phi(\phi(T)), \phi(T)]$.

% Discussion of Kamper 2016/2017 models
\cite{Kamper2017} has proposed a simplified Bayesian model that learns to jointly segment and cluster an audio waveform $\mathbf y = [y_1 \cdots y_T]$ into word-like unit $\mathbf z = z_1 \cdots z_K$. To that end, they employed an \textit{acoustic embedding} to map audio segments of variable duration into a fixed dimensional representation: $\mathbf x_i = f(\mathbf y_{t_1^i:t_2^i})$, where $i \in \{1, \cdots, M\}$ and $1 \leq t_1^i < t_2^i \leq T$. and $t_1^i \leq t_2^{i-1}, \forall i \in \{2, \cdots, M\}$. In the case where the entire audio consisting of spoken word, their system can be applied with full coverage and thus $t_1^i = t_2^{i-1}$. 

Their algorithm then iterates between finding a cluster assignment given segmentation and adjusting the segmentation given a cluster assignment. For a fixed segmentation, the acoustic embeddings are then modeled by the following GMM with Dirichlet prior for the cluster prior distributions and spherical normal for the cluster means: 
\begin{align}\label{eq:generate_bes_gmm}
    & \mathbf \pi \sim \mathbf{Dir}(a/K \mathbf 1)\\
    & \mu_c \sim \mathcal N(\mu_0, \sigma_0^2 \mathbf 1) \\
    & z_i \sim \mathbf \pi \\
    & \mathbf x_i \sim \mathcal N(\mu_{z_i}, \sigma^2 \mathbf I).
\end{align}
% More about the Gibbs sampling approach related to the model
The model can be trained using EM algorithm with Gibbs sampling and assign clusters by comparing the posterior probabilities $p(z_i=k|x_i)$. Given the clustering assignment, the model then updates the segmentation by performing the following dynamic program, analogous to \ref{eq:subword_hmm_dp}:
\begin{align}\label{eq:bes_gmm_dp}
    \delta(t) &= \max_{s}\{\delta(t-s) p(y_{t-s+1:t}|z_{k(t-s+1)})\} \\
    &= \max_s \{\alpha(t-s) p(f(y_{t-s+1:t}))^s\}\\
    \phi(t) &= t - \arg \max_{s}\{\delta(t-s) p(y_{t-s+1:t}|z_{k(t-s+1)})\} \\
    &= \max_s \{\delta(t-s) p(f(y_{t-s+1:t}))^s\}.
\end{align}

% Discussion of Goddard et. al.'s system and prior works
\subsection{Speech-to-text translation}

% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that the IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% the IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively. 
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.

\section{Conclusion}
The conclusion goes here.





% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


\appendices
\section{Proof of the First Zonklar Equation}
Appendix one text goes here.

% you can choose not to have a title for an appendix
% if you want by leaving the argument blank
\section{}
Appendix two text goes here.


% use section* for acknowledgment
\section*{Acknowledgment}


The authors would like to thank...


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliographystyle{IEEEtran}
\bibliography{sp2019_refs}

% that's all folks
\end{document}


